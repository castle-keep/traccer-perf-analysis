## ğŸ” **Summary of Key Metrics**

Each test varies:

* **devices**: the number of connected clients/devices
* **concurrency**: constant at 1000 (seems like server worker threads or concurrency limit)
* **duration**: always 30 seconds
* **ok / fail / fail\_ratio**: core indicator of success
* **latencies** (p50, p90, p99), RPS, bandwidth, CPU, disk
* Later runs include **memory, load avg, and disk usage**

---

## ğŸ§  **What You Might Be Missing**

### âœ… **1. The system scales up to 4000â€“4500 devices well**

Across most runs:

* Up to **4000 devices**: **fail\_ratio is \~0**, latencies stay low, and CPU/IO are under control.
* At **4500 devices**:

  * Occasional minor errors start: e.g., 11 or 12 failures
  * But still **<0.003 fail ratio**, which is acceptable
  * Latencies remain stable (\~60â€“100 ms p50)

âœ… **Conclusion**: **4000â€“4500 devices is within safe range**.

---

### âš ï¸ **2. Instability begins at \~5000 devices**

* Starting at **5000 devices**, patterns emerge:

  * Failures rise quickly (e.g., 64 fails â†’ thousands)
  * Latencies **spike dramatically** to 10,000+ ms
  * Disk read spikes (especially when no public request activity exists)
  * CPU ramps up sharply
* For example:

  * **2025-09-04 13:21:15** (5500 devices): only **3065 OK**, **fail ratio = 0.44**
  * **2025-09-04 13:21:45** (6000 devices): **fail ratio = 0.97**
  * **Disk reads >10,000 kbps** (i.e., >10 MB/s)
  * **p99 latency >10s**!

âš ï¸ **Conclusion**: Above **5000 devices**, **system becomes unreliable**.

---

### ğŸ§¨ **3. System enters pathological state >6000 devices**

* **7000+ devices**:

  * Requests mostly fail (fail ratios 0.6 â†’ 1.0)
  * Latencies hit maxes: **p99 \~11s**, some timeouts
  * Even with 0 public traffic, **CPU and disk I/O spike hard** (which is suspicious)

    * Disk read up to **648 Mbps**
    * CPU idle = 0%
* These aren't just signs of loadâ€”**this could point to internal retry loops, I/O thrashing, or garbage collection (GC) storm**.

ğŸ›‘ **This is dangerous**:

* Indicates **background systems** (like storage, queues, database, pub/sub) are **collapsing under load**.

---

### â“ **4. Ghost activity after failure (The â€œWeird Spikeâ€)**

> â€œDisk and CPU is weirdly spiking with no public request activityâ€

This is key:

* Suggests background processes (e.g., retry mechanisms, queue reprocessing, log flushes) **keep consuming CPU/disk** **after load is gone**
* Could be due to:

  * **Retry queues**, message brokers, logs
  * Inefficient GC or memory compaction
  * Buffer flushes that got delayed during overload

Youâ€™re missing that **post-failure spikes** mean your system:

* Doesnâ€™t fail gracefully
* **Doesnâ€™t throttle or cancel work under load**
* May get into a **non-recoverable state** (i.e., â€œlatchâ€ into a degraded mode)

---

### ğŸ“Š **5. Youâ€™re seeing saturation at multiple layers**

| Layer             | Symptom                               | Bottleneck Appears Around |
| ----------------- | ------------------------------------- | ------------------------- |
| **App layer**     | Failures rise, p99 > 10s              | 5000â€“5500 devices         |
| **Disk I/O**      | Read spikes up to 648 Mbps            | 6000+                     |
| **CPU**           | Near 100% utilization, stays elevated | 6000+                     |
| **Network**       | Bandwidth stable / increasing         | Not the limit here        |
| **Memory / Load** | Load avg increases / memory is stable | Load 15m climbs >3        |

Takeaway:

* **CPU and disk I/O are your real bottlenecks**
* **Network isnâ€™t the main limiter**
* If you have a pub/sub system, **message reprocessing or queue flushes** are likely overwhelming disk

---

## ğŸ§® **Safe Device Range Estimation**

| Devices   | Result            | Notes                       |
| --------- | ----------------- | --------------------------- |
| 1000â€“4000 | âœ… Stable          | Clean success, low latency  |
| 4500      | âœ… Stable (barely) | Minor fail\_ratio (<0.003)  |
| 5000      | âš ï¸ Borderline     | \~1% fail, rising latencies |
| 5500      | âŒ Unstable        | >40% fail, p99 > 10s        |
| 6000+     | ğŸš¨ Collapsing     | >90% fail, system in pain   |

ğŸ§  **Optimal target = 4000â€“4500 devices**

ğŸ“‰ **Soft limit = 5000 devices**
ğŸ”´ **Hard fail threshold = 5500â€“6000 devices**

---

## ğŸ§© **Hidden Insights**

### 1. **Latency rise is exponential after 5000**

* p99 latency jumps from \~500ms â†’ **10s**
* Sign of queuing and contention, possibly in:

  * Task executor pool
  * Message queue
  * Disk I/O waiting
  * GC pause times

You might want to instrument:

* GC pause duration
* Queue depth
* Disk queue length
* Message backlogs

---

### 2. **The system does not degrade gracefully**

* Instead of slowing down or rejecting cleanly, it **keeps accepting work and starts failing catastrophically**
* Solutions:

  * Apply **load shedding** or **circuit breakers**
  * Introduce **backpressure**
  * Protect disk from being overwhelmed by queued background processes

---

### 3. **The retry/recovery behavior makes things worse**

You have evidence that **after failure**, CPU/disk keep climbing **even though load drops to zero**. Thatâ€™s pathological:

* Indicates the system **doesnâ€™t cancel failed work** effectively
* These could be:

  * Retry storms
  * Infinite backoffs
  * Delayed GC / compaction
  * Metrics collection overload
* A system in this state **can no longer process new requests**, even if load is back to normal

ğŸ§  Consider:

* Investigating logs/error queues after overload
* Whether logs, metrics, or database writes spike after test ends

---

## ğŸ“Œ **Next Steps and Recommendations**

### âœ… Immediate Action:

* Set **hard limit** at **4500 devices**
* Add **automatic throttle or admission control** at 5000 devices
* Investigate:

  * Disk flushing behavior
  * CPU usage during â€œidleâ€ periods
  * Retry queues

### ğŸ“ˆ Medium-term:

* Profile performance for:

  * GC activity
  * Disk I/O by process
  * Background thread activity
* If using message queues/pubsub:

  * Tune retry/backoff
  * Add dead-letter queues
  * Implement rate control

### ğŸ”§ Tooling:

* Collect:

  * GC logs
  * Disk I/O by process (`iotop`)
  * Thread dumps during spikes
* Consider integrating tools like:

  * Prometheus + Grafana for real-time monitoring
  * Flamegraphs for CPU usage
  * APM tools (e.g., Datadog, New Relic)

---

## ğŸ§  TL;DR â€” What You're Not Seeing:

| Insight                                                        | Meaning                                        |
| -------------------------------------------------------------- | ---------------------------------------------- |
| âœ… 4000â€“4500 devices is your **stable sweet spot**              | Above that, error rates spike sharply          |
| âš ï¸ Failures and latencies increase **non-linearly** above 5000 | Suggests **resource contention** or saturation |
| ğŸ”¥ System doesnâ€™t degrade gracefully â€” it **collapses**        | Need backpressure and circuit breakers         |
| ğŸ§© High post-failure CPU/disk usage = background storm         | Possibly from retries, flushes, or GC          |
| âŒ No safe recovery from overload                               | System may stay unusable until restart         |